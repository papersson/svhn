{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Street View House Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.io import loadmat\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Online data loader\n",
    "A single image contains multiple labels, so the images are first cropped into separate digits according to the boundary boxes defined in digitStruct.mat. The data loader reads images from storage until we have full batch, and then it yields the batch for processing on the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineLoader:\n",
    "    def __init__(self, batch_size=32, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.matdata = h5py.File('lazy_data/train/digitStruct.mat')\n",
    "        self.size = self.matdata['/digitStruct/name'].size\n",
    "        self.indices = list(range(self.size))\n",
    "            \n",
    "    def __iter__(self):\n",
    "        \"\"\"  Generate batches. Optionally shuffle at start of each epoch. \"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indices = torch.randperm(self.size)\n",
    "            \n",
    "        n = 0\n",
    "        all_xs, all_ys = [], []\n",
    "        for i in self.indices:\n",
    "            xs, ys = self._load(i)\n",
    "            n += len(ys)\n",
    "            all_xs.append(xs)\n",
    "            all_ys.append(ys)\n",
    "            if n >= self.batch_size:\n",
    "                n = 0\n",
    "                x = torch.cat(all_xs)[:self.batch_size]\n",
    "                y = torch.cat(all_ys)[:self.batch_size]\n",
    "                y[y==10] = 0\n",
    "                all_xs, all_ys = [], []\n",
    "                yield x / 255, y.type(torch.LongTensor)\n",
    "    \n",
    "    def _load(self, i):\n",
    "        \"\"\" Load a .png image and separate digits by cropping according to the boundary boxes. The\n",
    "        images are also resized to maintain a consistent tensor shape. \"\"\"\n",
    "        filename = get_name(i, self.matdata)\n",
    "        boxdata = get_box_data(i, self.matdata)\n",
    "        img = read_image(f'lazy_data/train/{filename}')\n",
    "        xs = []\n",
    "        for j, label in enumerate(boxdata['label']):\n",
    "            top = int(boxdata['top'][j])\n",
    "            left = int(boxdata['left'][j])\n",
    "            height = int(boxdata['height'][j])\n",
    "            width = int(boxdata['width'][j])\n",
    "            xs.append(T.Resize([32, 32])(T.functional.crop(img, top, left, height, width)))\n",
    "        return torch.stack(xs), torch.as_tensor(boxdata['label'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "# https://stackoverflow.com/questions/41176258/h5py-access-data-in-datasets-in-svhn\n",
    "def get_box_data(index, hdf5_data):\n",
    "    \"\"\"\n",
    "    get `left, top, width, height` of each picture\n",
    "    :param index:\n",
    "    :param hdf5_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    meta_data = dict()\n",
    "    meta_data['height'] = []\n",
    "    meta_data['label'] = []\n",
    "    meta_data['left'] = []\n",
    "    meta_data['top'] = []\n",
    "    meta_data['width'] = []\n",
    "\n",
    "    def print_attrs(name, obj):\n",
    "        vals = []\n",
    "        if obj.shape[0] == 1:\n",
    "            vals.append(obj[0][0])\n",
    "        else:\n",
    "            for k in range(obj.shape[0]):\n",
    "                vals.append(int(hdf5_data[obj[k][0]][0][0]))\n",
    "        meta_data[name] = vals\n",
    "\n",
    "    box = hdf5_data['/digitStruct/bbox'][index]\n",
    "    hdf5_data[box[0]].visititems(print_attrs)\n",
    "    return meta_data\n",
    "\n",
    "def get_name(index, hdf5_data):\n",
    "    \"\"\" Get file path to image. \"\"\"\n",
    "    name = hdf5_data['/digitStruct/name']\n",
    "    return ''.join([chr(v[0]) for v in hdf5_data[name[index][0]].value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "Below is a fairly simple ResNet implementation consisting of some number or same-size ResBlocks, followed by two fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_blocks=5):\n",
    "        super().__init__()\n",
    "        in_channels = 3\n",
    "        conv_dim = 64\n",
    "        pool_size = 2\n",
    "        image_size = 32\n",
    "        conv_out = image_size // pool_size\n",
    "        fc_in = int(conv_out * conv_out * conv_dim)\n",
    "        fc_dim = 512\n",
    "        n_classes = 10\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, conv_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.res_blocks = nn.ModuleList([ResBlock(conv_dim, conv_dim) for _ in range(num_blocks)])\n",
    "        self.max_pool = nn.MaxPool2d(pool_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(fc_in, fc_dim)\n",
    "        self.fc2 = nn.Linear(fc_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity # Res connection\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The online dataloader is very slow compared to the offline approach, so below is just a demonstration that the online approach works (the models learns since it keeps improving the loss). This is far from an optimal solution.\n",
    "\n",
    "I have not applied data augmentation here since that would make this solution even slower, but to implement it I would follow the same approach as in the offline solution: convert the tensors into PIL images, then apply some transformations and convert the images back to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-aa0cdcf243e1>:6: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  self.matdata = h5py.File('lazy_data/train/digitStruct.mat')\n",
      "<ipython-input-3-aa0cdcf243e1>:76: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  return ''.join([chr(v[0]) for v in hdf5_data[name[index][0]].value])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 Loss: 0.072 Acc: 0.0625\n",
      "Batch: 100 Loss: 0.070 Acc: 0.19492574257425743\n",
      "Batch: 200 Loss: 0.064 Acc: 0.29322139303482586\n",
      "Batch: 300 Loss: 0.056 Acc: 0.38932724252491696\n",
      "Batch: 400 Loss: 0.050 Acc: 0.4661783042394015\n",
      "Batch: 500 Loss: 0.045 Acc: 0.5225174650698603\n",
      "Batch: 600 Loss: 0.042 Acc: 0.5645278702163061\n",
      "Batch: 700 Loss: 0.039 Acc: 0.5974054921540656\n",
      "Batch: 800 Loss: 0.037 Acc: 0.6240246566791511\n",
      "Batch: 900 Loss: 0.035 Acc: 0.6439372918978913\n",
      "Batch: 1000 Loss: 0.033 Acc: 0.661245004995005\n",
      "Batch: 1100 Loss: 0.032 Acc: 0.6753235694822888\n",
      "Batch: 1200 Loss: 0.031 Acc: 0.6872398001665279\n",
      "Batch: 1300 Loss: 0.030 Acc: 0.6985491929285165\n",
      "Batch: 1400 Loss: 0.029 Acc: 0.7083110278372591\n",
      "Batch: 1500 Loss: 0.028 Acc: 0.7170011658894071\n",
      "Batch: 1600 Loss: 0.027 Acc: 0.7252498438475953\n",
      "Batch: 1700 Loss: 0.027 Acc: 0.7324368018812464\n",
      "Batch: 1800 Loss: 0.026 Acc: 0.7389297612437534\n",
      "Batch: 1900 Loss: 0.026 Acc: 0.7451834560757497\n",
      "Batch: 2000 Loss: 0.025 Acc: 0.7503591954022989\n",
      "Batch: 2100 Loss: 0.025 Acc: 0.7555033317467873\n",
      "Batch: 2200 Loss: 0.024 Acc: 0.7598108814175375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.02414551179239617, 0.7613890129522108)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_one_epoch(dataloader, model, optimizer, loss_function, device, should_update=True):\n",
    "    model.to(device)\n",
    "    loss_tot, correct_tot = 0, 0\n",
    "    N = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        N += len(y)\n",
    "        out = model(x)\n",
    "        loss = loss_function(out, y)\n",
    "        if should_update:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_tot += loss.item()\n",
    "        _, y_hat = torch.max(out.data, 1)\n",
    "        correct_tot += (y_hat == y).sum().item()\n",
    "        if i % 100 == 0:\n",
    "            print(f'Batch: {i} Loss: {loss_tot / N:.3f} Acc: {correct_tot / N}')\n",
    "\n",
    "    return loss_tot / N, correct_tot / N\n",
    "\n",
    "online = OnlineLoader()\n",
    "net = ResNet(num_blocks=1)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "run_one_epoch(online, net, optimizer, loss_function, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
