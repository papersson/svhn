{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Street View House Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Solution using offline dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from PIL import ImageEnhance\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataloader\n",
    "I follow the Pytorch convention where a Dataset object handles all data transformations (shuffling, splitting, augmentation), and a Dataloader object is responsible for generating batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\" Dataset containing (X, y) pairs.\n",
    "    Args:\n",
    "        data_src (str | Tuple): The data source, which can either be a file path\n",
    "        or a (X, y) pair.\n",
    "        \n",
    "    Attributes:\n",
    "        augment: Boolean that indicates whether dataset should return augmented image.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_src):\n",
    "        if isinstance(data_src, str):\n",
    "            data = loadmat(data_src)\n",
    "            self.X = torch.from_numpy(data['X']).permute(3, 2, 0, 1) # [height, width, channels, batch_size]-> [batch_size, channels, height, width]\n",
    "            self.X = self.X / 255\n",
    "            self.y = torch.from_numpy(data['y'].astype(np.int64)).squeeze()\n",
    "            self.y[self.y == 10] = 0\n",
    "        else:\n",
    "            self.X, self.y = data_src\n",
    "        self.augment = False\n",
    "        \n",
    "    def shuffle(self):\n",
    "        \"\"\" Randomly shuffle dataset. \"\"\"\n",
    "        shuffled_idxs = torch.randperm(len(self))\n",
    "        self.X = self.X[shuffled_idxs]\n",
    "        self.y = self.y[shuffled_idxs]\n",
    "        \n",
    "    def split(self, ratio):\n",
    "        \"\"\" Randomly split dataset into two parts. \"\"\"\n",
    "        self.shuffle()\n",
    "        split_point = int(ratio * len(self))\n",
    "        train_split = (self.X[:split_point], self.y[:split_point])\n",
    "        val_split = (self.X[split_point:], self.y[split_point:])\n",
    "        return Dataset(train_split), Dataset(val_split)\n",
    "\n",
    "    def augment_data(self):\n",
    "        \"\"\" Apply data augmentation. \"\"\"\n",
    "        self.augment = True\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\" Allow indexing and slicing, e.g. data[3], data[5:9]. Either return\n",
    "        original data or transformed (augmented) data. \"\"\"\n",
    "        if not self.augment:\n",
    "            return self.X[i], self.y[i]\n",
    "        \n",
    "        N, _, _, _ = self.X[i].shape\n",
    "        if N > 1:\n",
    "            return self._transform_batch(self.X[i]), self.y[i]\n",
    "        else:\n",
    "            return self._transform(self.X[i]), self.y[i]\n",
    "\n",
    "    def _transform_batch(self, batch):\n",
    "        \"\"\" Transform batch with probability p=0.5. \"\"\"\n",
    "        p = 0.5\n",
    "        if np.random.rand() < p:\n",
    "            return batch\n",
    "        else:\n",
    "            return torch.stack([self._transform(x) for x in batch])\n",
    "    \n",
    "    def _transform(self, x):\n",
    "        \"\"\" Transform image by applying rotation, color augmentation, and gaussian blur. \"\"\"\n",
    "        # Convert tensor to PIL image\n",
    "        img = T.ToPILImage()(x)\n",
    "\n",
    "        # Rotate between -60 and 60 degrees\n",
    "        k = 30\n",
    "        angle = np.random.randint(k)\n",
    "        img = T.functional.rotate(img, angle)\n",
    "\n",
    "        # Color transform\n",
    "        jitter = T.ColorJitter(brightness=.5, hue=.3)\n",
    "        img = jitter(img)\n",
    "\n",
    "        # Blur\n",
    "        kernel_size = 2*(np.random.randint(11) // 2) + 1 # Odd kernel between 1 and 11\n",
    "        img = T.functional.gaussian_blur(img, kernel_size)\n",
    "        return T.ToTensor()(img)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        N, C, W, H = tuple(self.X.shape)\n",
    "        return f'Dataset(N={N}, C={C}, W={W}, H={H})\\nLabels: {self.y.unique()}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    \"\"\" A data loader responsible for generating batches of data for training.\n",
    "    \n",
    "    Args:\n",
    "        dataset (Dataset): A Dataset object.\n",
    "        batch_size (int): The number of data points in a batch.\n",
    "        shuffle (boolean): Whether to shuffle the data at start of epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size=32, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "            \n",
    "    def __iter__(self):\n",
    "        \"\"\"  Generate batches. Optionally shuffle at start of each epoch. \"\"\"\n",
    "        if self.shuffle:\n",
    "            self.dataset.shuffle()\n",
    "        for i in range(len(self.dataset) // self.batch_size + 1):\n",
    "            start = i * self.batch_size\n",
    "            end = (i + 1) * self.batch_size\n",
    "            X, y = self.dataset[start:end]\n",
    "            yield X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model training\n",
    "Below follows a simple ResNet implementation consisiting of some number of same-size ResBlocks, followed by two fully connected layers.\n",
    "\n",
    "I have also implemented a Learner class to handle training-related tasks (data preparation, the training loop, validation-training-evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_blocks=5):\n",
    "        super().__init__()\n",
    "        in_channels = 3\n",
    "        conv_dim = 64\n",
    "        pool_size = 2\n",
    "        image_size = 32\n",
    "        conv_out = image_size // pool_size\n",
    "        fc_in = int(conv_out * conv_out * conv_dim)\n",
    "        fc_dim = 512\n",
    "        n_classes = 10\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, conv_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.res_blocks = nn.ModuleList([ResBlock(conv_dim, conv_dim) for _ in range(num_blocks)])\n",
    "        self.max_pool = nn.MaxPool2d(pool_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(fc_in, fc_dim)\n",
    "        self.fc2 = nn.Linear(fc_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity # Res connection\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, loss_function, optimizer, data, testdata):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model\n",
    "        self.model.to(self.device)\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.data = data\n",
    "        self.testdata = testdata\n",
    "        \n",
    "    def validate(self, num_epochs, augment=False, batch_size=32, shuffle=True, split_ratio=0.8, save_best=False):\n",
    "        \"\"\" Split data into training and validation sets. Train on training set, validate on validation set.\"\"\"\n",
    "        \n",
    "        # Set up train and validation data\n",
    "        train_set, val_set = self.data.split(split_ratio)\n",
    "        if augment:\n",
    "            train_set.augment_data()\n",
    "        train_loader = Dataloader(train_set, batch_size=batch_size, shuffle=shuffle)\n",
    "        val_loader = Dataloader(val_set, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Training loop\n",
    "        self._run(num_epochs, (train_loader, val_loader), save_best=True, validate=True)\n",
    "                    \n",
    "    def train(self, num_epochs, augment=False, batch_size=32, shuffle=True, save_best=True):\n",
    "        \"\"\" Train on full dataset, i.e. on both training and validation data. \"\"\"\n",
    "        if augment:\n",
    "            self.data.augment()\n",
    "        full_dataloader = Dataloader(self.data, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "        # Training loop\n",
    "        self._run(num_epochs, (full_dataloader), save_best=True, validate=False)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\" Evaluate on test set. \"\"\"\n",
    "        testloader = Dataloader(self.testdata, shuffle=False)\n",
    "        test_loss, test_acc = self._run_one_epoch(testloader, should_update=False)\n",
    "        print(f'Test loss: {test_loss:.3f}\\nTest accuracy: {test_acc:.3f}')\n",
    "    \n",
    "    def _run(self, num_epochs, data, save_best, validate=True):\n",
    "        \"\"\" Prepare data and run the training loop. \"\"\"\n",
    "        \n",
    "        if validate:\n",
    "            train_loader, val_loader = data\n",
    "            heading = f'Epoch\\tTrain loss\\tVal loss\\tTrain acc\\tVal acc'\n",
    "        else:\n",
    "            train_loader = data\n",
    "            heading = f'Epoch\\tTrain loss\\tTrain acc'\n",
    "            \n",
    "        best_acc = 0.0\n",
    "        print(heading)\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train(True)\n",
    "            train_loss, train_acc = self._run_one_epoch(train_loader)\n",
    "            \n",
    "            if validate:\n",
    "                self.model.train(False)\n",
    "                val_loss, val_acc = self._run_one_epoch(val_loader, should_update=False)\n",
    "                result_str = f'{epoch + 1}\\t{train_loss:.3f}\\t\\t{val_loss:.3f}\\t\\t{train_acc:.3f}\\t\\t{val_acc:.3f}'\n",
    "            else:\n",
    "                result_str = f'{epoch + 1}\\t{train_loss:.3f}\\t\\t{train_acc:.3f}'\n",
    "            print(result_str)\n",
    "\n",
    "            if save_best:\n",
    "                acc = val_acc if validate else train_acc\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    torch.save(self.model.state_dict(), 'model.pt')\n",
    "                    \n",
    "    def _run_one_epoch(self, dataloader, should_update=True):\n",
    "        \"\"\" Training loop for single epoch. \"\"\"\n",
    "        loss_tot, correct_tot = 0, 0\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[0].to(self.device), batch[1].to(self.device)\n",
    "            out = self.model(X)\n",
    "            loss = self.loss_function(out, y)\n",
    "            if should_update:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            loss_tot += loss.item()\n",
    "            _, y_hat = torch.max(out.data, 1)\n",
    "            correct_tot += (y_hat == y).sum().item()\n",
    "        \n",
    "        return loss_tot / len(dataloader), correct_tot / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tTrain loss\tVal loss\tTrain acc\tVal acc\n",
      "1\t0.038\t\t0.018\t\t0.592\t\t0.819\n",
      "2\t0.018\t\t0.014\t\t0.818\t\t0.870\n",
      "3\t0.015\t\t0.012\t\t0.849\t\t0.889\n",
      "4\t0.014\t\t0.014\t\t0.867\t\t0.864\n",
      "5\t0.013\t\t0.011\t\t0.878\t\t0.901\n",
      "6\t0.012\t\t0.010\t\t0.886\t\t0.904\n",
      "7\t0.011\t\t0.010\t\t0.896\t\t0.908\n",
      "8\t0.010\t\t0.010\t\t0.901\t\t0.909\n",
      "9\t0.010\t\t0.009\t\t0.906\t\t0.913\n",
      "10\t0.009\t\t0.009\t\t0.910\t\t0.922\n",
      "11\t0.009\t\t0.009\t\t0.913\t\t0.919\n",
      "12\t0.009\t\t0.009\t\t0.918\t\t0.920\n",
      "13\t0.009\t\t0.008\t\t0.919\t\t0.929\n",
      "14\t0.008\t\t0.009\t\t0.921\t\t0.921\n",
      "15\t0.008\t\t0.008\t\t0.923\t\t0.931\n",
      "16\t0.008\t\t0.009\t\t0.925\t\t0.923\n",
      "17\t0.008\t\t0.008\t\t0.928\t\t0.930\n",
      "18\t0.008\t\t0.008\t\t0.929\t\t0.930\n",
      "19\t0.007\t\t0.008\t\t0.932\t\t0.930\n",
      "20\t0.007\t\t0.008\t\t0.932\t\t0.935\n",
      "21\t0.007\t\t0.007\t\t0.934\t\t0.936\n",
      "22\t0.007\t\t0.008\t\t0.935\t\t0.932\n",
      "23\t0.007\t\t0.008\t\t0.937\t\t0.931\n",
      "24\t0.007\t\t0.008\t\t0.936\t\t0.934\n",
      "25\t0.006\t\t0.008\t\t0.940\t\t0.934\n",
      "26\t0.006\t\t0.007\t\t0.940\t\t0.935\n",
      "27\t0.006\t\t0.008\t\t0.939\t\t0.929\n",
      "28\t0.006\t\t0.008\t\t0.942\t\t0.936\n",
      "29\t0.006\t\t0.008\t\t0.945\t\t0.936\n",
      "30\t0.006\t\t0.008\t\t0.945\t\t0.935\n",
      "31\t0.006\t\t0.008\t\t0.945\t\t0.937\n",
      "32\t0.006\t\t0.007\t\t0.946\t\t0.941\n",
      "33\t0.006\t\t0.008\t\t0.946\t\t0.936\n",
      "34\t0.005\t\t0.008\t\t0.948\t\t0.936\n",
      "35\t0.006\t\t0.008\t\t0.948\t\t0.936\n",
      "36\t0.005\t\t0.008\t\t0.948\t\t0.937\n",
      "37\t0.005\t\t0.007\t\t0.950\t\t0.937\n",
      "38\t0.005\t\t0.008\t\t0.953\t\t0.933\n",
      "39\t0.005\t\t0.008\t\t0.950\t\t0.937\n",
      "40\t0.005\t\t0.008\t\t0.952\t\t0.937\n",
      "Test loss: 0.007\n",
      "Test accuracy: 0.940\n"
     ]
    }
   ],
   "source": [
    "net = ResNet(num_blocks=7)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "dataset = Dataset('data/train_32x32.mat')\n",
    "testset = Dataset('data/test_32x32.mat')\n",
    "learner = Learner(net, loss_function, optimizer, dataset, testset)\n",
    "\n",
    "num_epochs = 40\n",
    "learner.validate(num_epochs, save_best=True, augment=True)\n",
    "learner.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
